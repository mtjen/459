---
title: "Max Tjen: 459 Homework 3"
author: "Max Tjen"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: paper
    highlight: textmate
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(comment=NA)
options(width = 70)
```


## Setup

```{r load_packages, message = FALSE, warning = FALSE}
library(nlme)
library(splines)
library(tidyverse)

theme_set(theme_bw())
```

## Data Import

```{r, message = FALSE}
pbc_data <- read_csv("/Users/mtjen/Desktop/459/homework/hw2/pbc.csv")
```


# Question 1
```{r}
q1_data <- pbc_data |> 
  select(id, year)

# find patients
patient_ids <- unique(pbc_data$id)

# initialize array for number of visits
num_visits <- c()

# iterate through patients and see number of visits
for (id_val in patient_ids) {
  temp <- q1_data |> filter(id == id_val)               # data for patient
  unique_visits <- unique(temp$year)                    # unique year values
  num_unique <- length(unique_visits)                   # amount of unique years
  num_visits <- append(num_visits, num_unique)          # append number of visits
}

# visualize via histogram
hist(num_visits,
     main = "Frequency of Number of Visits per Patient",
     breaks = 0:max(unique(num_visits)),
     xlab = "Number of Visits",
     ylab = "Number of Patients", 
     xlim = c(0, 20),
     ylim = c(0, 50),
     labels = TRUE)
```

Here, we create a histogram to visualize the distribution of the number of repeated measurements per patient. From this, we can see that the distribution is somewhat normal with the most frequent amount of visits being 4. The distribution is somewhat normally distributed, but there's also a bit of a uniform distribution from 0 to 9/10 visits. There is also a right skew with there being some patients with lots of visits.


# Question 2
```{r}
# relevel drug variable
pbc_data$drug = factor(pbc_data$drug, levels = c("placebo", "D-penicil"))

# complex model
m1_bean <- lme(prothrombin ~ (year + I(year^2)) * (sex + drug) + ns(age, 3) +
                 ns(age, 3) : (sex + drug), 
               data = pbc_data,
               random = ~ 1 | id, subset = prothrombin < 18)

# simple model
m2_bean <- lme(prothrombin ~ (year + I(year^2)) + (sex + drug), 
               data = pbc_data,
               random = ~ 1 | id, subset = prothrombin < 18)
```

## Part 1
```{r}
# p value
summary(m2_bean)

# confidence interval
intervals(m2_bean, level = 0.95)
```

Because we added levels for `drug`, the reference/baseline level is placebo. With this, the regression coefficient that we are looking at is `drugD-penicil`, which represents if a patient has received D-penicil or not. The coefficient value is -0.097 with a p-value of 0.299 and a 95% confidence interval of (-0.281, 0.087). The model estimates that if a patient is given D-penicil, their prothrombin value will be 0.097 seconds smaller than a patient given the placebo with all other variables held constant. However, the 95% confidence interval includes 0 and is (-0.281, 0.087). With this, we can't be certain about the effect direction, so receiving D-penicil may actually increase prothrombin rather than reduce. This is reinforced by the p-value being 0.299, which means that `drugD-penicil` is statistically insignificant. Given this, we can conclude that the `drug` coefficient may be 0 and that it doesn't make a significant impact within this model.

## Part 2

In this instance, a likelihood ratio test would not be valid. This is because a likelihood ratio test requires the models to be fit using the MLE method. These models' fit methods were not specified to use MLE, so they were fit with the function's default method of REML. Furthermore, the degrees of freedom specified is wrong, as it should be the difference in degrees between the two models. In this instance, m1_bean has 20 degrees of freedom and m2_bean has 7 degrees of freedom, so the likelihood ratio test should use 13 degrees of freedom [20 - 7].

```{r}
# complex model
m1_bean_fix <- lme(prothrombin ~ (year + I(year^2)) * (sex + drug) + ns(age, 3) +
                     ns(age, 3) : (sex + drug), 
                   data = pbc_data,
                   random = ~ 1 | id, subset = prothrombin < 18,
                   method = "ML")

# simple model
m2_bean_fix <- lme(prothrombin ~ (year + I(year^2)) + (sex + drug), 
                   data = pbc_data,
                   random = ~ 1 | id, subset = prothrombin < 18,
                   method = "ML")

# likelihood ratio test
anova(m1_bean_fix, m2_bean_fix)
```

By running the likelihood ratio test, we can see that the p-value when comparing these models is 0.125. This means that it's above our alpha level of 0.05, so the result is not statistically significant and we can't reject the null hypothesis. With this, we can conclude that the two models perform similarly and that the complex model isn't significantly better than the simplified model.


# Question 3

## Part 1
```{r}
m3_bean <- lme(prothrombin ~ (year + I(year^2)) + (sex + drug), 
          data = pbc_data,
          random = ~ year | id, subset = prothrombin < 18)

m3_bean
```

The treatment effect from the random intercept model m2_bean is -0.097 and for the new model, it is -0.131. This means that with all other variables held constant, the predicted prothrombin value of a patient receiving the treatment from the m2_bean model will be 0.034 larger (-0.097--0.131) than the predicted value from the m3_bean model.

## Part 2

```{r}
# likelihood ratio test
anova(m2_bean, m3_bean)

# get p value
l2 <- m3_bean$logLik
l1 <- m2_bean$logLik
ts <- -2 * (l1 - l2)
true_p <- 1 - pchisq(ts, df = 1)
true_p
```

To help determine which model is better, we used a likelihood ratio test via anova(). Since it's random effects being compared, we also have to get the true p-value as the anova test doesn't report a correct p-value. By running a likelihood ratio test on m2_bean and m3_bean, we can see that m3_bean appears to be the better model. This can be seen by the log likelihood for each model and is a statistically significant result by the p-value being less than 0.05.

## Part 3

No, this would not be a valid approach overall. By running the anova test, Mr Bean will get the correct log likelihood values, however the p-value from the test will be incorrect. This is because he is testing for changes in random effects, so the true p-value will have to be manually calculated.


# Queston 4

## Part 1
```{r}
# create gobb model
m1_gobb <- gls(prothrombin ~ (year + I(year^2)) + (sex + drug), 
               data = pbc_data,
               corr = corCompSymm(form = ~ 1 | id), subset = prothrombin < 18)

m1_gobb
```

For this, we used the gls model with compound symmetry correlation structure using the same grouping structure. We chose this because the m2_bean model is a random intercept model so we know to use the compound symmetry correlation structure because random intercept models impose the compound symmetry correlation structure.

## Part 2

Even though the two models have the same results, the predictions for each will not necessarily be the same. Because the coefficients are the same, each model will make predictions with the same slope. However, some variables like `drug` likely impact each patient differently so a subject-specific model will likely fit better. This effect is accounted for in m2_bean by the random intercept, which helps the model account for each subject. As such, the `drug` effect can be interpreted differently because of how one model is more tailored to the subjects.

## Part 3

While the regression coefficients of the two models are the same, the two models' predicted responses differ through their intercepts. In general, the blue prediction lines of the m2_bean model is fits the data better than the red prediction lines of the m1_gobb model. Both of them predict the same slope as it is based on the predictions of the average patient's profile. The intercepts are different though, because the m2_bean model is subject-specific as it takes the observed data into account and is a weighted average of the population mean. In doing so, the intercept is adjusted to fit the data better and reduces variance overall.


# Session Information

```{r}
sessionInfo()
```

